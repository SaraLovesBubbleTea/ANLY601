{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "#   Importing libraries  #\n",
    "##########################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as py\n",
    "import time\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#   Generate some training    #\n",
    "#      data from a GMM        #\n",
    "###############################\n",
    "def gen_gmm_data(n = 999, plot=False):\n",
    "    # Fixing seed for repeatability\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # Parameters of a normal distribuion\n",
    "    mean_1 = [0, 2] ; mean_2 = [2, -2] ; mean_3 = [-2, -2]\n",
    "    mean = [mean_1, mean_2, mean_3] ; cov = [[1, 0], [0, 1]]  \n",
    "    \n",
    "    # Setting up the class probabilities\n",
    "    n_samples = n\n",
    "    pr_class_1 = pr_class_2 = pr_class_3 = 1/3.0\n",
    "    n_class = (n_samples * np.array([pr_class_1,pr_class_2, pr_class_3])).astype(int)\n",
    "  \n",
    "    # Generate sample data\n",
    "    for i in range(3):\n",
    "        x1,x2 = np.random.multivariate_normal(mean[i], cov, n_class[i]).T\n",
    "        if (i==0):\n",
    "            xs = np.array([x1,x2])\n",
    "            cl = np.array([n_class[i]*[i]])\n",
    "        else: \n",
    "            xs_new = np.array([x1,x2])\n",
    "            cl_new = np.array([n_class[i]*[i]])\n",
    "            xs = np.concatenate((xs, xs_new), axis = 1)\n",
    "            cl = np.concatenate((cl, cl_new), axis = 1)\n",
    "    \n",
    "    # Plot?\n",
    "    if plot:\n",
    "        py.scatter(xs[:1,:],xs[1:,:], c = cl)\n",
    "\n",
    "    # One hot encoding classes\n",
    "    y = pd.Series(cl[0].tolist())\n",
    "    y = pd.get_dummies(y).as_matrix() \n",
    "\n",
    "    # Normalizing data (prevents overflow errors)     \n",
    "    mu = xs.mean(axis = 1)\n",
    "    std = xs.std(axis = 1)\n",
    "    xs = (xs.T - mu) / std\n",
    "    \n",
    "    return xs, y, cl\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#  Generate data for network    #\n",
    "#################################\n",
    "X, Y, cl = gen_gmm_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Neural Network activation function sigmoid, 1 layer with 1 node and sgd_w_reg_momentum as regularization scheme.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#    Hidden Units     #\n",
    "#######################\n",
    "ReLU = np.vectorize(lambda z: np.fmax(0,z))\n",
    "sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "softmax = lambda z: np.exp(z)/(np.sum(np.exp(z),axis=1))[:,np.newaxis]\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%\n",
    "#  Utility Functions  #\n",
    "# #####################\n",
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat, axis=1)\n",
    "\n",
    "\n",
    "def error_rate(Y_hat, cl):\n",
    "    prediction = predict(Y_hat)\n",
    "    return np.mean(prediction != cl)\n",
    "\n",
    "\n",
    "def cost(Y_hat, Y):\n",
    "    tot = Y * np.log(Y_hat)\n",
    "    return -tot.sum()\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#   1- Hidden Layer Sigmoid Network   #\n",
    "####################################\n",
    "def forward(X,parameters, drop_out = 1):\n",
    "    # Unpacking parameters    \n",
    "    W,b1,V,b2 = parameters\n",
    "    \n",
    "    # Forward pass\n",
    "    a1 = X.dot(W) + b1\n",
    "    #H = ReLU(a1) * drop_out\n",
    "    H = sigmoid(a1) * drop_out\n",
    "    a2 = H.dot(V) + b2\n",
    "    Y_hat = softmax(a2)\n",
    "    return H,Y_hat\n",
    "\n",
    "#%%%%%%%%%%%%%%%\n",
    "#   Gradient   #\n",
    "################\n",
    "\n",
    "def grad(X,H,Y,Y_hat,parameters):  \n",
    "    # Unpacking parameters    \n",
    "    W,b1,V,b2 = parameters\n",
    "    # Gradients - ReLU\n",
    "    dV = H.T.dot(Y_hat - Y)\n",
    "    db2 = (Y_hat - Y).sum(axis=0)\n",
    "    \n",
    "    #dW = X.T.dot(((Y_hat - Y).dot(V.T) * (H > 0))) \n",
    "    #db1 = ((Y_hat - Y).dot(V.T) * (H > 0)).sum(axis=0)\n",
    "    \n",
    "    # Gradients - sigmoid\n",
    "    dW = X.T.dot((Y_hat-Y).dot(V.T) * (H * (1 - H)))\n",
    "    db1 = (Y_hat-Y).dot(V.T) * (H * (1 - H)).sum(axis=0)\n",
    "    \n",
    "    \n",
    "    #return {'dV':dV,'db2':db2,'dW':dW,'db1':db1}\n",
    "    return dW,db1,dV,db2\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#   Parameter Update: Momentum + Regularization   #\n",
    "###################################################\n",
    "def parameter_update(parameters,  grads, \n",
    "                     momentum_params = [0,0,0,0], \n",
    "                     lr = 1, reg = 0, alpha = 0):\n",
    "    # Unpacking parameters            \n",
    "    W,b1,V,b2 = parameters\n",
    "    dW,db1,dV,db2 = grads\n",
    "    vW,vb1,vV,vb2 = momentum_params\n",
    "    \n",
    "    # Momentum update\n",
    "    vW  = alpha * vW -  lr * (dW + reg*W)\n",
    "    vb1 = alpha * vb1 - lr * (db1 + reg*b1)\n",
    "    vV  = alpha * vV -  lr * (dV + reg*V)\n",
    "    vb2 = alpha * vb2 - lr * (db2 + reg*b2)\n",
    "    momentum_params = [vW,vb1,vV,vb2] \n",
    "    \n",
    "    # Parameter updates\n",
    "    W  = W  + vW\n",
    "    b1 = b1 + vb1\n",
    "    V  = V  + vV\n",
    "    b2 = b2 + vb2\n",
    "    parameters =[W,b1,V,b2]\n",
    "\n",
    "             \n",
    "    return parameters, momentum_params\n",
    "\n",
    "\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#      Building the model     #\n",
    "###############################\n",
    "def run_model(X, Y, cl,\n",
    "              nodes_in_hidden_layer = 1,\n",
    "              num_dim = 2,     # <- number of dimensions here it is 2: x1,x2\n",
    "              num_classes = 3, # <- number of classes in the problem\n",
    "              iterations = 1000,\n",
    "              regularization_include = False,\n",
    "              momentum_include = False,\n",
    "              drop_out_include = False):    \n",
    "\n",
    "    \n",
    "    ###################################\n",
    "    #   Initial values for network    #\n",
    "    ###################################\n",
    "    # Intialize weights\n",
    "    np.random.seed(123)\n",
    "    W = np.random.randn(num_dim * nodes_in_hidden_layer).reshape(num_dim,nodes_in_hidden_layer)\n",
    "    b1 = 0\n",
    "    V = np.random.randn(num_classes * nodes_in_hidden_layer).reshape(nodes_in_hidden_layer,num_classes)\n",
    "    b2 = 0\n",
    "    parameters = [W,b1,V,b2]\n",
    "    \n",
    "    \n",
    "    # Hyperparameters \n",
    "    lr = 0.0001 # learning rate\n",
    "    reg = 0.01 * regularization_include\n",
    "    \n",
    "    # Momentum parameters\n",
    "    alpha = 0.9 * momentum_include\n",
    "    vV = 0\n",
    "    vb2 = 0\n",
    "    vW = 0\n",
    "    vb1 = 0\n",
    "    momentum_params = [vW,vb1,vV,vb2]    \n",
    "    \n",
    "    # Place holder for losses\n",
    "    losses = []\n",
    "    errors = []    \n",
    "   \n",
    "    ###################\n",
    "    #   Run the model #\n",
    "    ###################\n",
    "    for i in range(0,iterations):\n",
    "        # -- Drop Out Mask --\n",
    "        # When !=1 then ddrop rate is 12.5%  (~0.5/4 = 12.5%)\n",
    "        # Short cut to include drop_out \n",
    "        drop_out =  (1 - drop_out_include) + drop_out_include * np.round(1-np.random.rand(nodes_in_hidden_layer)/4) \n",
    "        \n",
    "        # -- Forward propoagation --\n",
    "        H,Y_hat = forward(X,parameters,drop_out)\n",
    "        \n",
    "        # -- Backward propagation --\n",
    "        # Gradient calculation\n",
    "        grads_in = grad(X,H,Y,Y_hat,parameters)\n",
    "        # Parameter update\n",
    "        new_params, new_mom_param = parameter_update(parameters, grads_in, \n",
    "                             momentum_params, alpha = alpha, \n",
    "                             lr = lr, reg = reg)\n",
    "        \n",
    "        # -- Updating values --\n",
    "        H,Y_hat = forward(X,new_params, drop_out)\n",
    "        parameters = new_params\n",
    "        momentum_params = new_mom_param\n",
    "        # Prediction and Error rate            \n",
    "        errs_i = error_rate(Y_hat, cl) ; errors.append(errs_i)\n",
    "        loss_i = cost(Y_hat, Y); losses.append(loss_i)\n",
    "        if ((i % 25) == 0):    \n",
    "            print(\n",
    "            '''\n",
    "            ---- Iteration {i} ----\n",
    "            Error rate : {er}\n",
    "            Loss: {loss}\n",
    "            '''.format(i= i, er = errs_i, loss = loss_i))\n",
    "    return {\"errors\":errs_i, \"loss\":losses, \"parameters\":parameters, 'pred_y':Y_hat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ---- Iteration 0 ----\n",
      "            Error rate : 0.6666666666666666\n",
      "            Loss: 1003.0934314581206\n",
      "            \n",
      "\n",
      "            ---- Iteration 25 ----\n",
      "            Error rate : 0.2702702702702703\n",
      "            Loss: 663.7146571690657\n",
      "            \n",
      "\n",
      "            ---- Iteration 50 ----\n",
      "            Error rate : 0.004004004004004004\n",
      "            Loss: 475.27557510568045\n",
      "            \n",
      "\n",
      "            ---- Iteration 75 ----\n",
      "            Error rate : 0.0\n",
      "            Loss: 377.9671818916887\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "sgd_w_reg_momentum = run_model(X,Y,cl, iterations = 100,\n",
    "                              regularization_include = True,\n",
    "                              momentum_include = True,\n",
    "                              drop_out_include = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8138687 ,  0.00233879,  0.18379252],\n",
       "       [ 0.81366779,  0.00234515,  0.18398706],\n",
       "       [ 0.81387348,  0.00233863,  0.18378789],\n",
       "       ..., \n",
       "       [ 0.22438697,  0.17651962,  0.59909341],\n",
       "       [ 0.22384071,  0.17706703,  0.59909226],\n",
       "       [ 0.22429472,  0.17661194,  0.59909335]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = sgd_w_reg_momentum['pred_y']\n",
    "pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
