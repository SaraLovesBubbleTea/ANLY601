---
title: "ANLY601 Assignment2"
author: "Xiner Ning"
date: "2/20/2020"
output: html_document

header-includes: 
  - \usepackage{tikz}
---
collaborators: Fan Liang
  
# Problem {.tabset .tabset-dropdown}  
  
## Question 1: Convexity  
  
1. f(x) is convex because all p-norm vectors is convex for p>=1, which is the same as p>0 since p can only be positive integer.   
2. f(d) is convex. Since k(d) is positive definite, it is convex so does it derivative. And linear combination of two convex functions is also convex.  
  
3. f(d) is not convex since multiplication of two convex function is not necessary convex.  
    
4, 5 are not convex functions.    





## Question 2: Kernel regression  
```{r}

mydata <- read.csv('kernel_regression_1.csv')
myx <- mydata$x
myy <- mydata$y

### exponential kernel 
expoKernel <- function(a,b){
  k <- exp(-3*abs(a-b))
  return (k)
}


kdeEstimateyX <- seq(-6,6,1)
ykernel_expo <- NULL


for(xesti in kdeEstimateyX){
  sum1 = sum(expoKernel(xesti,myx)*myy)
  sum2 = sum(expoKernel(xesti,myx))
  xkyk = c(xesti,sum1/sum2)
  ykernel_expo <- rbind(ykernel_expo,xkyk)
}

### radial basis kernel
radialKernel <- function(a,b){
  k <- exp(-2*(a-b)^2)
  return (k)
}


kdeEstimateyX <- seq(-6,6,1)
ykernel_radial <- NULL


for(xesti in kdeEstimateyX){
  sum1 = sum(radialKernel(xesti,myx)*myy)
  sum2 = sum(radialKernel(xesti,myx))
  xkyk = c(xesti,sum1/sum2)
  ykernel_radial <- rbind(ykernel_radial,xkyk)
}

### uniform kernel
uniformKernel <- function(a,b){
  k <- abs(a-b)<0.5
  return (k)
}


kdeEstimateyX <- seq(-6,6,1)
ykernel_uniform <- NULL


for(xesti in kdeEstimateyX){
  sum1 = sum(uniformKernel(xesti,myx)*myy)
  sum2 = sum(uniformKernel(xesti,myx))
  xkyk = c(xesti,sum1/sum2)
  ykernel_uniform <- rbind(ykernel_uniform,xkyk)
}
plot(myx,myy,xlab = "x", ylab = "y", col = 'green', cex = 2)
lines(ykernel_radial[,1],ykernel_radial[,2], col = 'red', lwd = 2)
lines(ykernel_expo[,1],ykernel_expo[,2], col = 'blue', lwd = 2)
lines(ykernel_uniform[,1],ykernel_uniform[,2], col = 'black', lwd = 2)

```
  
So we can see above, the smoothed curve from exponential, radial basis and uniform kernels are red, blue and black.They are not so much different from each other. 
  
## Question 4: Calculating the conjugate distribution  
1. 
Given $\mu$~N($\tau$,$v$), $\sigma^2$~IG($\alpha$,$\beta$), where X~N($\mu$,$\sigma^2$), we have $x|\mu,\sigma^2$ ~ N($\mu,\sigma^2$)  
$\mu|\sigma^2$ ~ N($\tau,v$)  
$\sigma^2$ ~ IG($\alpha$,$\beta$)  
  
so $\mu | x,\sigma^2$ ~ N($\frac{n\sigma^2}{n\sigma^2+n_0\sigma^2}\bar{x}+\frac{v}{n\sigma^2+v}\tau,n\sigma^2+v$)  
  
$\pi(\sigma^2|x,\mu) \propto$ Gamma($\alpha + \frac{n}{2},\beta+\frac{1}{2}\Sigma(x_i-\bar{x})^2+\frac{nn_0}{2(n+n_0)}(\bar{x}-\tau)^2$)  

  
3. $\pi(\lambda|y)$ = $\frac{Pr(\lambda,y)}{\pi(y)} = \frac{Pr(y|\lambda)\pi(\lambda)}{Pr(y)}$ = $\Pi\frac{\lambda^{y_i}e^{-\lambda}}{y_i!}\frac{\lambda^{\alpha-1}e^{\lambda/\beta}}{\tau(\alpha)\beta^\alpha}\propto \lambda^{n\bar{y}+\alpha -1}e^{-\lambda(n+1/\beta)}$   
so $\lambda | y$ ~ Gamma ($n\bar{y}+\alpha,\frac{\beta}{n\beta + 1}$)



  
## Question 5: Priors as regularizers  
Suppose we want to infer some parameter $\beta$ from some observed input-output pairs (x1,y1),(x2,y2)...,(xn, yn).
  
Assume $y_n = \beta x_n + \epsilon$, where $\epsilon$~N(0,$\sigma^2$). So the likelihood is  

$\Pi_{n=1}^{N}N(y_n|\beta x_n,\sigma^2)$  
  
If we regularise parameter $\beta$ by imposing the Gaussian prior $N(\beta | 0,\lambda^{-1})$ and combine the likelihood with the prior we have:  
$\Pi_{n=1}^{N}N(y_n|\beta x_n,\sigma^2)N(\beta | 0,\lambda^{-1})$  
  
After taking the logarithm and dropping some constans, we get  
$\Sigma_{n=1}^{N} -\frac{1}{\sigma^2}(y_n-\beta x_n)^2-\lambda\beta^2+constant$  
  
So we see that Gaussian prior can be interpreted as a L2 regularisation term.  
  
Similarly, the relationship L1 norm and Laplace prior are equivalent.  




## Question 6: General questions?  
  
1. Posterior distribution depends on the unknown parameter $\theta$ while the posterior predictive distribution is the integrand over the data generating mechanism and posterior distribution fora newly observed set of values, which is independent of the parameter $\theta$.  
  
In other words, posterior distribution predicts an unknown, random variable based on the observed data. On the other hand, predictive posterior distribution predicts furture data based on the observed data. It is used to predict new data values.  
  
2. I would use predictive posterior distribution to predict future values. The reasons are shown above.  
  
3. 
show $\mu_{MAP}$ -> $\mu_{MLE}$  
  
Find $\mu_{MLE}$:  
  
$P(x_1,..,x_n | \mu) = \Pi_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x_i-\mu)^2}{2\sigma^2})$  
  
$log(P(x_1,..,x_n | \mu)) = \Sigma_{i=1}^{n}log(\frac{1}{\sqrt{2\pi\sigma^2}})-\frac{(x_i-\mu)^2}{2\sigma^2} $  
  
$\frac{d log(P(x_1,..,x_n | \mu))}{2\mu} = \Sigma_{i=1}^{n} \frac{(x_i-\mu)}{\sigma^2} $  
  
Setting $\Sigma_{i=1}^{n} \frac{(x_i-\mu)}{\sigma^2}=0$, we get $\hat{\mu} = \frac{\Sigma_{i=1}^{n}x_i}{n}$  
  
Find $\mu_{MAP}$:  
  
We know that $P(x_1,..,x_n | \mu) = \Pi_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x_i-\mu)^2}{2\sigma^2})$
  
and we are given that $P(\mu)  = \frac{1}{\sqrt{2\pi\beta}}exp(-\frac{(\mu-\alpha)^2}{2\beta})$  
  
Based on Bayes Rule  
  
$P(\mu|x_1,...x_n) = \frac{P(x_1,..,x_n | \mu)P(\mu)}{P(x_1,...,x_n)}$  
  
Similar to the process of finding $\mu_{MLE}$, we take the log and take the derivative with respect to $\mu$,  
  
we have $\hat{\mu} = \frac{\sigma^2\alpha+\beta \space \Sigma_{i=1}^{n}x_i}{\sigma^2+n\beta} = \frac{\sigma^2\alpha}{\sigma^2+n\beta} +\frac{\frac{\Sigma_{i=1}^{n}x_i}{n}}{1+\frac{\sigma^2}{n\beta}}$   
  
So when n gets large, $\hat{\mu} = \frac{\Sigma_{i=1}^{n}x_i}{n}$   
  
We have shown that when n gets large, $\mu_{MAP}$ -> $\mu_{MLE}$ 

## Question 7: Programming a Gibbs Sampler  
From Q4, we see that the prosterior distributino of $\lambda$ is $\lambda | y$ ~ Gamma ($n\bar{y}+\alpha,\frac{\beta}{n\beta+1}$)  
  
```{r}
### Example Gibbs Sampler ###
# Summary stats from the sample
gibbs <- read.csv('gibbs.csv')
y <- gibbs$value

n <- length(y)
ybar <- mean(y)
alpha=30
beta=4
# Pre-allocation

lambda <- rep(NA, 11000)
# Constants
T <- 1000 # burnin

# Gibbs Sampler (mu, tau | data)
for(i in 2:11000) {

lambda[i] <- rgamma(n = 1, shape = n*ybar+alpha, scale = (beta)/(n*beta+1))
}
# Remove burnin

lambda <- lambda[-(1:T)]


plot(density(lambda))
```
  
## Interview Question  
SVM algorithms use a set of mathematical functions that are defined as kernel. Kernal function takes data as input and transform it into the required form. Common kernel functions are linear, RBF, sigmoid. 
  
If the training set is not linearly separable, we set a fat decision margin to allow a few mistakes. We pay a cost for each misclassfied example depending on how far it is from meeting the margin requirement. To implement this, we introduce slack variable. 
  
RBF kernel is used when choosing a non-linear decision boundary. So slack variables are not a problem when using this kernel.  
  
  






